{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple variables\n",
    "In previous example we had target functions with only one variable so that they could be aproximated by a hyphothesis function $h(x)=a+bx$. What if we had multiple variables instead, say hell of a lot of variables:\n",
    "\n",
    "$$h(x)=\\sum_{i=0}^{n}w_ix_i, \\quad \\text{where $x_0=1$}$$\n",
    "\n",
    "The letter **w** comes from the term **weight** which is used in machine learning to mean a constant multiplying a variable in the hyphothesis function. The first w ($w_0$) is also called **bias** because it's independent of a variable ($x_0$ is always $1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # for numerical operations\n",
    "import matplotlib.pyplot as plt # for visualization (plotting)\n",
    "\n",
    "# to display plots inline (with the rest of the output) rather than in a separate window\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a recap of the code from the previous part dealing with univariate target functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.99597457535\n",
      "2.00016983747\n"
     ]
    }
   ],
   "source": [
    "class Machine:\n",
    "    a = 0\n",
    "    b = 0\n",
    "    def h(self, x):\n",
    "        return self.a + self.b * x\n",
    "    \n",
    "def gradient_descent(machine, t, training_set, alpha, iterations):\n",
    "    a_values = []\n",
    "    b_values = []\n",
    "    n = training_set.size\n",
    "    for i in range(0, iterations):\n",
    "        a_values.append(machine.a)\n",
    "        b_values.append(machine.b)\n",
    "        da = 0\n",
    "        db = 0\n",
    "        for x in training_set:\n",
    "            da += machine.h(x) - t(x)\n",
    "            db += (machine.h(x) - t(x)) * x\n",
    "        machine.a -= alpha * da / n\n",
    "        machine.b -= alpha * db / n\n",
    "\n",
    "def trainer(t, n, alpha, iterations, seed):\n",
    "    np.random.seed(seed)\n",
    "    training_set = np.random.randint(-5, 6, n)\n",
    "    machine = Machine()\n",
    "    gradient_descent(machine, t, training_set, alpha, iterations)\n",
    "    print(machine.a)\n",
    "    print(machine.b)\n",
    "\n",
    "trainer(lambda x: 3 + 2 * x, n = 3, alpha = 0.2, iterations = 30, seed = 876)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will modify the code to handle target functions of multiple variables. We already have two for loops so we really don't want to add another loop to go through all the variables (we also don't want to write them all separately). So we'll introduce a litte bit of linear algebra to prevent the additional for loop.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w_j &= w_j - \\frac{\\alpha}{m}\\displaystyle\\sum_{i=1}^m(h(x_i)-t(x_i))x_i \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "def t_old(x): return 3 + 2 * x\n",
    "def t(x):\n",
    "    w = np.array([3,2])\n",
    "    return w.dot(x)\n",
    "\n",
    "print(t_old(4))\n",
    "print(t(np.array([1, 4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 2 # number of variables\n",
    "m = 3 # number of training samples\n",
    "\n",
    "class Machine:\n",
    "    def __init__(self, n):\n",
    "        self.w = np.zeros(n)\n",
    "    def h(self, x):\n",
    "        return np.sum(self.w * x)\n",
    "    \n",
    "def t(x):\n",
    "    #seed = 123\n",
    "    #np.random.seed(seed)\n",
    "    #w = np.random.randint(-5, 6, n)\n",
    "    w = [3,2]\n",
    "    return np.sum(w * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 -3]\n",
      " [ 1  0]\n",
      " [ 1  4]]\n",
      "[ 2.99597458  2.00016984]\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(machine, t, training_set, alpha, iterations):\n",
    "    m = training_set.shape[0]\n",
    "    n = training_set.shape[1]\n",
    "    values = [] # iterations x n \n",
    "    for i in range(0, iterations):\n",
    "        values.append(machine.w)\n",
    "        d = np.zeros(n)\n",
    "        for x in training_set:\n",
    "            d += (machine.h(x) - t(x)) * x\n",
    "        machine.w -= alpha * d / m\n",
    "\n",
    "def trainer(t, m, alpha, iterations, seed):\n",
    "    np.random.seed(seed)\n",
    "    training_set = np.random.randint(-5, 6, (m,1))\n",
    "    training_set = np.insert(training_set, 0, 1, axis=1)\n",
    "    print(training_set)\n",
    "    machine = Machine(n)\n",
    "    gradient_descent(machine, t, training_set, alpha, iterations)\n",
    "    print(machine.w)\n",
    "\n",
    "trainer(t, m = 3, alpha = 0.2, iterations = 30, seed = 876)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machine-learning]",
   "language": "python",
   "name": "conda-env-machine-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
